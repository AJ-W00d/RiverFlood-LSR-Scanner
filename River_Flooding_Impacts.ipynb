{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta \n",
    "import geopandas as gpd\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = datetime.today() + timedelta(days=1) \n",
    "date = date.strftime('%Y-%m-%d')\n",
    "\n",
    "date1 = datetime.today() - timedelta(days=30*365) \n",
    "date1 = date1.strftime('%Y-%m-%d')\n",
    "\n",
    "url = urllib.request.urlretrieve('https://mesonet.agron.iastate.edu/cgi-bin/request/gis/lsr.py?&type=FLOOD&sts='+date1+'T00:00Z&ets='+date+'T00:00Z&fmt=csv', r'report.csv')\n",
    "df = pd.read_csv('report.csv', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill remark column with '' in place of Nans\n",
    "df['TYPETEXT'] = df['TYPETEXT'].fillna('')\n",
    "\n",
    "#df = df.dropna()\n",
    "df = df[df['TYPETEXT'].str.contains(\"FLOOD\")]\n",
    "df = df[~df['TYPETEXT'].str.contains(\"FLASH FLOOD\")]\n",
    "df = df[~df['TYPETEXT'].str.contains(\"COASTAL FLOOD\")]\n",
    "#df = df[df.LON > -130]\n",
    "#df = df[df.LON < -50]\n",
    "\n",
    "df['LAT'] = df['LAT'].astype(float)\n",
    "#df = df[df.LAT > 20.00]\n",
    "\n",
    "\n",
    "#Change VALID2 time to datetime format\n",
    "df['VALID2'] = pd.to_datetime(df['VALID2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needs work, script is not actually filtering the LSRs\n",
    "\n",
    "\n",
    "\n",
    "#Filter to find most likely river flooding events.\n",
    "#df =  df[df['REMARK'].str.contains(\"closed|river|River\")]\n",
    "#df =  df[df['REMARK'].str.contains(\"River\")]\n",
    "#df.to_csv(\"River_Only.csv\")\n",
    "\n",
    "import re\n",
    "# Remove column name 'MAG'\n",
    "df.drop(['MAG'], axis=1)\n",
    "\n",
    "#keywords = [\"River\", \"stream\", \"overbank\", \"creek\", \"Creek\", \"Bayou\", \"bayou\", \"Banks\", \"banks\", \"Bank\", \"bank\", \"gage\", \"Gage\", \"road\"]\n",
    "# Function to check if any word in a text is in the list of keywords\n",
    "# Create a regular expression pattern from the list of keywords\n",
    "#pattern = '|'.join(keywords)\n",
    "\n",
    "#Fill remark column with '' in place of Nans\n",
    "df['REMARK'] = df['REMARK'].fillna('')\n",
    "\n",
    "\n",
    "# Search for rows containing any of the keywords\n",
    "#df = df[df['REMARK'].str.contains(pattern, flags=re.IGNORECASE)]\n",
    "\n",
    "df.to_csv(\"River_Only.csv\")\n",
    "\n",
    "#df = df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create geodataframe\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.LON, df.LAT), crs=\"EPSG:4326\")\n",
    "\n",
    "#Change Projection\n",
    "gdf = gdf.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['REMARK']=df['REMARK'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will download the latest ahps shapefile\n",
    "\n",
    "#Download data and save the zipped shapefile\n",
    "urllib.request.urlretrieve('https://water.noaa.gov/resources/downloads/shapefiles/national_shapefile_fcst_ffep.tgz', r'ahps.tgz')\n",
    "\n",
    "# For future refence, it appears that the national_shapefile_fcst_ffep.tgz file above has a few more sites than the observed file below.\n",
    "# Therefore, I opted to use the national_shapefile_fcst_ffep.tgz.\n",
    "\n",
    "#urllib.request.urlretrieve('https://water.noaa.gov/resources/downloads/shapefiles/national_shapefile_obs.tgz', r'ahps.tgz')\n",
    "#The file is zipped so now we have to unzip the file like we normally do with an AHPS Shapefile\n",
    "fname = 'ahps.tgz'\n",
    "if fname.endswith(\"ahps.tgz\"):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "elif fname.endswith(\"tar\"):\n",
    "    tar = tarfile.open(fname, \"r:\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every shapefile has a dbf file which is a table.We can now read that table and create a Pandas Dataframe with it.  \n",
    "table = gpd.read_file(r'national_shapefile_fcst_ffep.dbf')\n",
    "ahps = pd.DataFrame(table)\n",
    "#This will save it if you want to. It may be good to look at.\n",
    "ahps.to_csv('ahps.csv')\n",
    "#View the table just by using this line\n",
    "#ahps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ahps DataFrame to a GeoDataFrame, adding a geometry column from the 'Latitude' and 'Longitude' columns\n",
    "# We're converting the ahps DataFrame to a GeoDataFrame with point geometries to enable spatial operations\n",
    "\n",
    "ahps = gpd.GeoDataFrame(ahps, geometry=gpd.points_from_xy(ahps.Longitude, ahps.Latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "#Change Projection\n",
    "ahps = ahps.to_crs(epsg=3857)\n",
    "\n",
    "# Perform a spatial join between the GeoDataFrame gdf and the GeoDataFrame ahps, \n",
    "# keeping only the nearest match from ahps for each geometry in gdf.\n",
    "# This finds the nearest gage (point) for each geometry in gdf\n",
    "gdf_joined = gpd.sjoin_nearest(gdf, ahps, how='left')\n",
    "\n",
    "# Create a copy of the resulting GeoDataFrame gdf_joined\n",
    "# Creating a copy ensures that modifications to gdf won't affect gdf_joined\n",
    "gdf = gdf_joined.copy()\n",
    "\n",
    "# Convert the 'Latitude' and 'Longitude' columns to string data type\n",
    "# This conversion might be necessary for later string operations\n",
    "gdf = gdf.astype({'Latitude': 'str', 'Longitude': 'str'})\n",
    "\n",
    "# Create a new column 'geometry_ahps' by concatenating the 'Longitude' and 'Latitude' columns \n",
    "# and forming a Well-Known Text (WKT) representation of a point geometry\n",
    "# This creates a WKT representation of point geometries from the 'Latitude' and 'Longitude' columns\n",
    "gdf['geometry_ahps'] = \"POINT (\"+ gdf['Longitude']+\" \"+gdf['Latitude']+\")\"\n",
    "\n",
    "# Convert the 'geometry_ahps' column from WKT representation to Shapely geometries\n",
    "# Convert WKT strings to Shapely Point geometries\n",
    "gdf['geometry_ahps'] = gdf['geometry_ahps'].apply(wkt.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoSeries from the 'geometry_ahps' column and set its CRS to EPSG:4326\n",
    "gdf['geometry_ahps'] = gpd.GeoSeries(gdf['geometry_ahps'], crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure both geometry columns have the same CRS\n",
    "if gdf.crs != gdf['geometry_ahps'].crs:\n",
    "    gdf['geometry_ahps'] = gdf['geometry_ahps'].to_crs(gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distance between the 'geometry' and 'geometry_ahps' for each row in the GeoDataFrame\n",
    "# and store the result in a new column named 'distance'\n",
    "# This calculates the distance between each geometry in gdf and its corresponding geometry in 'geometry_ahps'\n",
    "gdf[\"distance\"] = gdf.apply(lambda row: row[\"geometry\"].distance(row[\"geometry_ahps\"]), axis=1)\n",
    "\n",
    "# Convert the distance from meters to miles and store the result in a new column named 'distance_miles'\n",
    "# Convert distances from meters to miles for convenience\n",
    "gdf[\"distance_miles\"] = gdf[\"distance\"]/1609\n",
    "\n",
    "#This will give us all the LSRs within 5 miles of a gage, which might be a good place to start.\n",
    "gdf = gdf[gdf['distance_miles'] < 5]\n",
    "\n",
    "gdf.to_csv('delete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "urllib.request.urlretrieve('https://water.noaa.gov/resources/downloads/reports/nwps_all_gauges_report.csv',r'nwps_all_gauges_report.csv')\n",
    "\n",
    "ahps = pd.read_csv('nwps_all_gauges_report.csv', index_col=False)\n",
    "\n",
    "ahps = ahps[ahps['usgs id'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'GaugeLID' and set it equal to the values in the 'nws shef id' column\n",
    "# Convert the values to strings and convert them to uppercase\n",
    "ahps['GaugeLID'] = ahps['nws shef id']\n",
    "ahps['GaugeLID'] = ahps['GaugeLID'].astype(str)\n",
    "ahps['GaugeLID'] = ahps['GaugeLID'].str.upper()\n",
    "\n",
    "# Keep only the 'GaugeLID' and 'usgs id' columns in the DataFrame\n",
    "# This is to retain only the necessary information for merging with the main DataFrame gdf\n",
    "ahps = ahps[['GaugeLID', 'usgs id', 'action stage', 'flood stage','moderate flood stage', 'major flood stage']]\n",
    "\n",
    "# Merge the main DataFrame gdf with the ahps DataFrame on the 'GaugeLID' column, using a left join\n",
    "# This adds the USGS ID information to the main DataFrame gdf based on the 'GaugeLID' column\n",
    "gdf = pd.merge(gdf, ahps, how='left', on=\"GaugeLID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'datetime_column' to datetime format for begin date.\n",
    "gdf['begin_date'] = pd.to_datetime(gdf['VALID2'], format='%Y/%m/%d %H:%M')\n",
    "gdf['begin_date']=gdf['begin_date']- pd.to_timedelta(1, unit='d')\n",
    "gdf['begin_date']=gdf['begin_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert the 'datetime_column' to datetime format for end date.\n",
    "gdf['end_date'] = pd.to_datetime(gdf['VALID2'], format='%Y/%m/%d %H:%M')\n",
    "gdf['end_date']=gdf['end_date']+ pd.to_timedelta(1, unit='d')\n",
    "gdf['end_date']=gdf['end_date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_csv('delete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_status(row):  \n",
    "    try:    \n",
    "        site = row['usgs id']\n",
    "        begin_date = row['begin_date']\n",
    "        end_date = row['end_date']\n",
    "\n",
    "        action = float(row['action stage'])\n",
    "        minor = float(row['flood stage'])\n",
    "        moderate = float(row['moderate flood stage'])\n",
    "        major = float(row['major flood stage'])\n",
    "\n",
    "        event_time = row['VALID2']\n",
    "\n",
    "        table = f'https://nwis.waterdata.usgs.gov/usa/nwis/uv/?cb_00065=on&format=rdb&site_no={site}&period=&begin_date={begin_date}&end_date={end_date}'\n",
    "        print(table)\n",
    "        df = pd.read_fwf(table, comment='#')\n",
    "        df = df.iloc[1:,0].str.split(\"\\t\", expand=True)\n",
    "        df.rename(columns={0:'Agency', 1:'USGSstationID', 2:'Date', 4:'Stage'}, inplace=True)\n",
    "        df['Stage'] = pd.to_numeric(df['Stage'], errors='coerce').dropna()\n",
    "        df['Date1'] = pd.to_datetime(df['Date'])\n",
    "        df['date'] = df['Date1'].dt.strftime('%Y-%m-%d, %H'+'00')\n",
    "\n",
    "        def f(row):\n",
    "            if row['Stage'] > major and major != -9999:\n",
    "                return 'major'\n",
    "            elif row['Stage'] > moderate and moderate != -9999:\n",
    "                return 'moderate'\n",
    "            elif row['Stage'] > minor and minor != -9999:\n",
    "                return 'minor'\n",
    "            elif row['Stage'] > action and action != -9999:\n",
    "                return 'action'\n",
    "            return 'none'\n",
    "\n",
    "        df['status'] = df.apply(f, axis=1) \n",
    "\n",
    "        event_row_index = (df['Date1'] - event_time).abs().idxmin()\n",
    "        event_row = df.loc[event_row_index]\n",
    "        return event_row['Stage'], event_row['status'], event_row['date']\n",
    "\n",
    "    except Exception as e:\n",
    "        return 'error: USGS', 'error: USGS', 'error: USGS'\n",
    "\n",
    "# Parallel processing using joblib\n",
    "num_cores = -1  # Uses all available cores\n",
    "results = Parallel(n_jobs=num_cores)(delayed(return_status)(row) for _, row in gdf.iterrows())\n",
    "\n",
    "# Assign results back to the dataframe\n",
    "gdf[['Event stage (ft)', 'Event status', 'Event date']] = pd.DataFrame(results, index=gdf.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_csv('river_impacts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV into a GeoDataFrame\n",
    "gdf = pd.read_csv('river_impacts.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Create lsr url for lsr\n",
    "#Change time to string so it can be used to make the url\n",
    "gdf['VALID'] = gdf['VALID'].astype(str)\n",
    "\n",
    "#Create URL\n",
    "gdf['lsr_url'] = 'https://mesonet.agron.iastate.edu/lsr/#'+gdf['WFO_left']+'/'+gdf['VALID']+'/'+gdf['VALID']+'/010010'\n",
    "\n",
    "gdf['VALID2'] = pd.to_datetime(gdf['VALID2'], errors='coerce')\n",
    "\n",
    "# Define a function to create the suggested comment for each row\n",
    "def create_suggested_comment(row):\n",
    "    return (\n",
    "        'There is potential for flooding at this location if the current flow were to materialize. \\n\\n' +\n",
    "        f'This is supported by an LSR (link below) that occurred on {row[\"VALID2\"].strftime(\"%Y-%m-%d\")} at {row[\"VALID2\"].strftime(\"%H:%M\")}\\n\\n' +\n",
    "        f'The nearby gage ({row[\"GaugeLID\"]}) on the {row[\"Waterbody\"]} reported a stage and status of {row[\"Event stage (ft)\"]} ft and {row[\"Event status\"]} flood.\\n\\n' +\n",
    "        row['lsr_url']\n",
    "    )\n",
    "\n",
    "# Apply the function to each row and create the 'suggested_comment' column\n",
    "gdf['suggested_comment'] = gdf.apply(create_suggested_comment, axis=1)\n",
    "\n",
    "# Convert 'VALDID2' column to string\n",
    "gdf['VALID2'] = gdf['VALID2'].dt.strftime('%Y-%m-%d')  # Or any format you prefer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two columns ended up being State, to fix this the 25th column was called directly to rename specifically\n",
    "gdf.rename(columns={gdf.columns[25]: \"Gage State\"}, inplace=True)\n",
    "\n",
    "#Renaming columns\n",
    "gdf.rename(columns={\n",
    "    'VALID': 'LSR Date/Time', 'LAT': 'Lat', 'LON': 'Lon', 'WFO_left': 'WFO', 'TYPETEXT': 'Event Type',\n",
    "    'CITY': 'City', 'COUNTY': 'County', 'STATE':'State', 'SOURCE': 'Source', 'REMARK': 'Remark',\n",
    "    'GaugeLID': 'Nearest Gage', 'Location': 'Gage Location', 'Waterbody': 'Gage Waterbody', \n",
    "    'Action': 'Action Threshold', 'Flood': 'Minor Threshold', 'Moderate': 'Moderate Threshold',\n",
    "    'Major': 'Major Threshold', 'URL': 'Gage URL', 'distance_miles': 'LSR to Gage Distance (miles)',\n",
    "    'usgs id': 'USGS ID', 'Event stage (ft)': 'Event Stage (ft)', 'Event status': 'Event_Status',\n",
    "    'Event date': 'Gage Observation Date/Time', 'lsr_url': 'LSR Link', 'suggested_comment': 'Suggested Comment'}, inplace=True)\n",
    "\n",
    "#Deletes any leading/trailing spaces in column names\n",
    "gdf.columns = gdf.columns.str.strip()\n",
    "# List of columns to drop\n",
    "gdf.drop(columns=[j for j in [\n",
    "    'Unnamed: 0', 'VALID2', 'MAG', 'TYPECODE', 'UGC', 'UGCNAME', 'QUALIFIER', \n",
    "    'index_right', 'Status', 'Latitude', 'Longitude', 'Forecast', 'FcstTime', \n",
    "    'FcstIssunc', 'Units', 'LowThresh', 'LowThreshU', 'WFO_right', 'HDatum', \n",
    "    'PEDTS', 'SecValue', 'SecUnit', 'geometry_ahps', 'distance', 'action stage', \n",
    "    'flood stage', 'moderate flood stage', 'major flood stage', 'begin_date', 'end_date'] \n",
    "    if j in gdf.columns], inplace=True)\n",
    "\n",
    "# Define mapping for event status values\n",
    "status_mapping = {\n",
    "    'action': 1,\n",
    "    'minor': 2,\n",
    "    'moderate': 3,\n",
    "    'major': 4\n",
    "}\n",
    "\n",
    "# Create the new column with mapped values, defaulting to 0 for unmapped values\n",
    "gdf['Event Status Code'] = gdf['Event_Status'].map(status_mapping).fillna(0).astype(float)\n",
    "\n",
    "#Filter to only minor and higher\n",
    "gdf = gdf[gdf['Event Status Code'] > 1]\n",
    "\n",
    "gdf.to_csv('gdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to only minor and higher\n",
    "gdf = gdf[gdf['Event Status Code'] > 1]\n",
    "\n",
    "gdf['Event Status Code'] = gdf['Event Status Code'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david.smith\\AppData\\Local\\Temp\\ipykernel_12668\\1927496774.py:8: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  lsrs.to_file(\"lsrs_shapefile.shp\")\n"
     ]
    }
   ],
   "source": [
    "#Set the projection\n",
    "lsrs = gpd.GeoDataFrame(gdf, geometry=gpd.points_from_xy(gdf.Lon, gdf.Lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Reproject to Web Mercator (EPSG:3857)\n",
    "lsrs = lsrs.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Save the GeoDataFrame as a shapefile\n",
    "lsrs.to_file(\"lsrs_shapefile.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save as a GeoPackage\n",
    "lsrs.to_file(\"lsrs_feature_layer.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
